# End-to-End-LLM-Based-Medical-chatbot-RAG
This is End to End Rag project


# ğŸ©º End-to-End LLM-Based Medical Chatbot using RAG

This project is an end-to-end Medical Question Answering Chatbot built using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG).  
The system generates accurate and context-aware medical responses by grounding LLM outputs with information retrieved from a medical knowledge base.

---

## ğŸ“Œ Project Motivation

Large Language Models can generate fluent responses, but in sensitive domains like healthcare they may hallucinate or provide unreliable information.  
To address this, I implemented a Retrieval-Augmented Generation (RAG) architecture where relevant medical documents are retrieved first and then passed as context to the LLM for response generation.

This approach improves:
- Factual accuracy  
- Domain relevance  
- Trustworthiness of responses  

---

## ğŸ§  Architecture Overview

User Query  
â†’ Flask API  
â†’ LangChain RAG Pipeline  
â†’ Query Embedding  
â†’ Semantic Search using Pinecone  
â†’ Context Injection into LLM  
â†’ Final Medical Response  

---

## ğŸ› ï¸ Tech Stack

- Language Model: OpenAI GPT  
- RAG Framework: LangChain  
- Vector Database: Pinecone  
- Backend API: Flask  
- Embeddings: Sentence Transformers  
- Document Processing: PyPDF  
- Deployment: AWS EC2  
- Secrets Management: Environment Variables (.env)

---

## âœ¨ Key Features

- Semantic search over medical documents using vector embeddings  
- Retrieval-Augmented Generation to reduce hallucinations  
- Domain-specific medical knowledge base  
- Secure API key and credential management  
- Deployable both locally and on AWS  

---

## ğŸ” Conversational RAG with Memory

This project has been extended from a basic knowledge-based RAG system
to a conversational RAG architecture using LangChain's
ConversationBufferMemory.

### Key Features
- Maintains full chat history across turns
- Supports follow-up medical questions
- Uses ConversationalRetrievalChain
- Non-agentic, deterministic control flow

### Architecture
User â†’ Flask API â†’ Conversational Retrieval Chain
â†’ (Conversation Buffer Memory + Pinecone Retriever + LLM)
â†’ Context-aware medical response

-----------
## ğŸ¤– Agentic RAG Architecture

This project has been upgraded from a conversational RAG system to an
Agentic RAG architecture using LangChain agents.

### Key Enhancements
- ReAct-style reasoning loop
- LLM-driven decision making
- Retriever exposed as a tool
- Conversation memory preserved
- Conditional medical knowledge retrieval

### Architecture
User â†’ Flask API â†’ LLM Agent
â†’ (Conversation Memory + Pinecone Retriever Tool)
â†’ Grounded Medical Response

User (Medical Question)
â”‚
â”œâ”€â”€â–º Flask API
â”‚     â””â”€â”€ Inference Endpoint
â”‚
â”œâ”€â”€â–º LLM Agent (LangChain Agent)
â”‚     â”œâ”€â”€ ReAct Reasoning Loop
â”‚     â”œâ”€â”€ LLM-driven Decision Making
â”‚     â”‚     â”œâ”€â”€ Retrieve medical knowledge?
â”‚     â”‚     â”œâ”€â”€ Refine the query?
â”‚     â”‚     â””â”€â”€ Answer directly?
â”‚     â”‚
â”‚     â”œâ”€â”€ Conversation Memory
â”‚     â”‚     â”œâ”€â”€ ConversationBufferMemory
â”‚     â”‚     â””â”€â”€ Chat History
â”‚     â”‚
â”‚     â””â”€â”€ Retriever Tool
â”‚           â”œâ”€â”€ Pinecone Vector Database
â”‚           â”œâ”€â”€ Medical Embeddings
â”‚           â””â”€â”€ Semantic Similarity Search
â”‚
â”œâ”€â”€â–º Retrieved Medical Context
â”‚
â”œâ”€â”€â–º LLM Generation
â”‚     â””â”€â”€ Grounded Medical Answer
â”‚
â””â”€â”€â–º Final Response


